{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# Dino Fine-Tuning - Simple"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install -q transformers>=4.40 accelerate peft"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!rm -rf Dino-Training\n!git clone https://github.com/RekitRex21/Dino-Training.git\n%cd Dino-Training"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import json, os\ndata = []\nfor f in os.listdir('.'):\n  if f.endswith('.json') or f.endswith('.jsonl'):\n    with open(f) as fp:\n      if f.endswith('.jsonl'): data.extend([json.loads(l) for l in fp])\n      else: data.extend(json.load(fp))\nprint(f\"Loaded {len(data)} examples\")"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nm = 'microsoft/Phi-3-mini-4k-instruct'\ntok = AutoTokenizer.from_pretrained(m, trust_remote_code=True)\ntok.pad_token = tok.eos_token\nmod = AutoModelForCausalLM.from_pretrained(m, device_map='cpu', trust_remote_code=True)\nprint('Model loaded (CPU mode)')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["texts = [f\"### Instruction\\n{d.get('instruction','')}\\n\\n### Input\\n{d.get('input','')}\\n\\n### Response\\n{d['output']}\" for d in data]"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from datasets import Dataset\nds = Dataset.from_dict({'text': texts[:10]}) # Use 10 examples for quick test\ndef tok_fn(ex): return tok(ex['text'], truncation=True, max_length=256)\ntokenized = ds.map(tok_fn, batched=True)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from peft import LoraConfig, get_peft_model\nlc = LoraConfig(r=4, lora_alpha=8, target_modules=['q_proj'], lora_dropout=0.1, bias='none', task_type='CAUSAL_LM')\nmod = get_peft_model(mod, lc)\nmod.print_trainable_parameters()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import TrainingArguments, Trainer\nargs = TrainingArguments(output_dir='./out', num_train_epochs=1, per_device_train_batch_size=1, logging_steps=1)\ntrainer = Trainer(model=mod, args=args, train_dataset=tokenized)\ntrainer.train()\nprint('Training done!')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('For full training: increase examples, epochs, use GPU colab')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
