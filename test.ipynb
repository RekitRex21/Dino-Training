{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# Dino Fine-Tuning"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install -q transformers>=4.40 accelerate peft bitsandbytes"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!rm -rf Dino-Training\n!git clone https://github.com/RekitRex21/Dino-Training.git\n%cd Dino-Training"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import json, os\ndata = []\nfor f in os.listdir('.'):\n  if f.endswith('.json') or f.endswith('.jsonl'):\n    with open(f) as fp:\n      if f.endswith('.jsonl'): data.extend([json.loads(l) for l in fp])\n      else: data.extend(json.load(fp))\nprint(f\"Loaded {len(data)} examples\")"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import AutoModelForCausalLM, AutoTokenizer\nm = \"microsoft/Phi-3-mini-4k-instruct\"\ntok = AutoTokenizer.from_pretrained(m, trust_remote_code=True)\ntok.pad_token = tok.eos_token\nmod = AutoModelForCausalLM.from_pretrained(m, device_map='auto', load_in_4bit=True, trust_remote_code=True)\nprint(\"Model loaded\")"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["texts = [f\"### Instruction\\n{d.get('instruction','')}\\n\\n### Input\\n{d.get('input','')}\\n\\n### Response\\n{d['output']}\" for d in data]"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from datasets import Dataset\nds = Dataset.from_dict({'text': texts})\ntok_fn = lambda ex: tok(ex['text'], truncation=True, max_length=512)\ntokenized = ds.map(tok_fn, batched=True)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from peft import LoraConfig, get_peft_model\nlc = LoraConfig(r=8, lora_alpha=16, target_modules=['q_proj','k_proj','v_proj'], lora_dropout=0.1, bias='none', task_type='CAUSAL_LM')\nmod = get_peft_model(mod, lc)\nmod.print_trainable_parameters()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import TrainingArguments, Trainer\nargs = TrainingArguments(output_dir='./out', num_train_epochs=3, per_device_train_batch_size=4, learning_rate=2e-4, fp16=True, logging_steps=5)\ntrainer = Trainer(model=mod, args=args, train_dataset=tokenized)\ntrainer.train()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mod = mod.merge_and_unload()\nmod.save_pretrained('./rex-dino-v1')\ntok.save_pretrained('./rex-dino-v1')\nprint('Done! Download rex-dino-v1 folder')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
